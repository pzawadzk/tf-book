{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\" # shortcut URL\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115394,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = encoded.shape[0] * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(19, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for d in dataset.take(2):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1\n",
      "  0 22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1\n",
      "  4  8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24\n",
      " 17  0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23\n",
      " 10 15  3 13  0], shape=(101,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[ 5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1  0\n",
      " 22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1  4\n",
      "  8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24 17\n",
      "  0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23 10\n",
      " 15  3 13  0  4], shape=(101,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for d in dataset.take(2):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=1004, shape=(128, 100), dtype=int64, numpy=\n",
      "array([[15,  0, 11, ...,  7,  2,  0],\n",
      "       [ 9,  0, 13, ...,  1,  4,  8],\n",
      "       [ 5, 13,  7, ...,  4,  2,  0],\n",
      "       ...,\n",
      "       [ 9,  1, 14, ...,  3,  8,  9],\n",
      "       [13,  8,  2, ...,  9,  0,  2],\n",
      "       [ 8, 24, 27, ...,  5,  8,  7]])>, <tf.Tensor: id=1005, shape=(128, 100), dtype=int64, numpy=\n",
      "array([[ 0, 11,  5, ...,  2,  0, 18],\n",
      "       [ 0, 13, 22, ...,  4,  8,  0],\n",
      "       [13,  7, 23, ...,  2,  0,  4],\n",
      "       ...,\n",
      "       [ 1, 14, 15, ...,  8,  9,  0],\n",
      "       [ 8,  2,  6, ...,  0,  2,  3],\n",
      "       [24, 27,  7, ...,  8,  7,  2]])>)\n"
     ]
    }
   ],
   "source": [
    "print_ds(dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for el in dataset:\n",
    "    print (el)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for el in dataset:\n",
    "    print (el)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7842.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size //128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateless RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "#model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "#history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=4)\n",
    "#model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = preprocess([\"Thus the country to the str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_classes() got an unexpected keyword argument 'steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-e706813525ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: predict_classes() got an unexpected keyword argument 'steps'"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, model, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new, batch_size=1, steps=1)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "     \n",
    "    id_int = char_id.eval(session=tf.compat.v1.Session())\n",
    "    return tokenizer.sequences_to_texts(id_int)\n",
    "         #tokenizer.sequences_to_texts(char_id.numpy())[0] # version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = next_char(\"You are  ss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, model, n_chars=10, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, model, temperature)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am perharded'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_text(\"I am\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statefull RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model_size = 1 \n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.repeat().batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "# for el in dataset:\n",
    "#     print(el)\n",
    "#     break\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ds(ds, count = 1):\n",
    "    c = 0\n",
    "    for el in ds:\n",
    "        print(el)\n",
    "        c +=1\n",
    "        if c >= count:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "slice_size = train_size // batch_size\n",
    "window_length = 101\n",
    "n_steps = 100\n",
    "\n",
    "\n",
    "datasets = []\n",
    "for i in range(batch_size):\n",
    "    data_slice = tf.data.Dataset.from_tensor_slices(encoded[i*slice_size:(i+1)*slice_size])\n",
    "    data_slice = data_slice.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    data_slice = data_slice.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(data_slice)\n",
    "    \n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=27560, shape=(32, 100, 39), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.]]], dtype=float32)>, <tf.Tensor: id=27561, shape=(32, 100), dtype=int64, numpy=\n",
      "array([[ 5,  8,  7, ...,  3, 13,  0],\n",
      "       [ 1, 31, 19, ...,  6,  1,  0],\n",
      "       [ 5,  9,  5, ..., 11, 12,  0],\n",
      "       ...,\n",
      "       [ 2,  0,  5, ...,  3,  9,  7],\n",
      "       [25,  3,  7, ...,  1, 25,  1],\n",
      "       [ 1,  7,  0, ...,  7,  2,  3]])>)\n"
     ]
    }
   ],
   "source": [
    "print_ds(dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 313 steps\n",
      "Epoch 1/40\n",
      "313/313 [==============================] - 71s 226ms/step - loss: 2.6398\n",
      "Epoch 2/40\n",
      "313/313 [==============================] - 69s 222ms/step - loss: 2.2153\n",
      "Epoch 3/40\n",
      "313/313 [==============================] - 70s 224ms/step - loss: 2.0615\n",
      "Epoch 4/40\n",
      "313/313 [==============================] - 70s 223ms/step - loss: 1.9728\n",
      "Epoch 5/40\n",
      "313/313 [==============================] - 69s 222ms/step - loss: 1.9126\n",
      "Epoch 6/40\n",
      "313/313 [==============================] - 70s 224ms/step - loss: 1.8698\n",
      "Epoch 7/40\n",
      "313/313 [==============================] - 69s 221ms/step - loss: 1.8379\n",
      "Epoch 8/40\n",
      "313/313 [==============================] - 70s 224ms/step - loss: 1.8112\n",
      "Epoch 9/40\n",
      "313/313 [==============================] - 69s 221ms/step - loss: 1.7889\n",
      "Epoch 10/40\n",
      "313/313 [==============================] - 69s 222ms/step - loss: 1.7729\n",
      "Epoch 11/40\n",
      "313/313 [==============================] - 70s 224ms/step - loss: 1.7582\n",
      "Epoch 12/40\n",
      "313/313 [==============================] - 70s 222ms/step - loss: 1.7457\n",
      "Epoch 13/40\n",
      "313/313 [==============================] - 70s 224ms/step - loss: 1.7340\n",
      "Epoch 14/40\n",
      "313/313 [==============================] - 70s 223ms/step - loss: 1.7241\n",
      "Epoch 15/40\n",
      "313/313 [==============================] - 69s 222ms/step - loss: 1.7164\n",
      "Epoch 16/40\n",
      "313/313 [==============================] - 70s 225ms/step - loss: 1.7080\n",
      "Epoch 17/40\n",
      "313/313 [==============================] - 69s 222ms/step - loss: 1.7006\n",
      "Epoch 18/40\n",
      "313/313 [==============================] - 70s 224ms/step - loss: 1.6950\n",
      "Epoch 19/40\n",
      "313/313 [==============================] - 69s 221ms/step - loss: 1.6901\n",
      "Epoch 20/40\n",
      "313/313 [==============================] - 69s 220ms/step - loss: 1.6826\n",
      "Epoch 21/40\n",
      "313/313 [==============================] - 70s 224ms/step - loss: 1.6787\n",
      "Epoch 22/40\n",
      "313/313 [==============================] - 69s 222ms/step - loss: 1.6737\n",
      "Epoch 23/40\n",
      "313/313 [==============================] - 69s 221ms/step - loss: 1.6703\n",
      "Epoch 24/40\n",
      "313/313 [==============================] - 70s 224ms/step - loss: 1.6663\n",
      "Epoch 25/40\n",
      "313/313 [==============================] - 69s 222ms/step - loss: 1.6637\n",
      "Epoch 26/40\n",
      "313/313 [==============================] - 70s 223ms/step - loss: 1.6579\n",
      "Epoch 27/40\n",
      "313/313 [==============================] - 69s 222ms/step - loss: 1.6565\n",
      "Epoch 28/40\n",
      "313/313 [==============================] - 70s 222ms/step - loss: 1.6532\n",
      "Epoch 29/40\n",
      "313/313 [==============================] - 70s 223ms/step - loss: 1.6501\n",
      "Epoch 30/40\n",
      "313/313 [==============================] - 69s 221ms/step - loss: 1.6476\n",
      "Epoch 31/40\n",
      "313/313 [==============================] - 70s 224ms/step - loss: 1.6444\n",
      "Epoch 32/40\n",
      "313/313 [==============================] - 69s 222ms/step - loss: 1.6418\n",
      "Epoch 33/40\n",
      "313/313 [==============================] - 70s 223ms/step - loss: 1.6378\n",
      "Epoch 34/40\n",
      "313/313 [==============================] - 70s 224ms/step - loss: 1.6369\n",
      "Epoch 35/40\n",
      "313/313 [==============================] - 69s 222ms/step - loss: 1.6349\n",
      "Epoch 36/40\n",
      "313/313 [==============================] - 69s 220ms/step - loss: 1.6321\n",
      "Epoch 37/40\n",
      "313/313 [==============================] - 70s 223ms/step - loss: 1.6305\n",
      "Epoch 38/40\n",
      "313/313 [==============================] - 69s 220ms/step - loss: 1.6294\n",
      "Epoch 39/40\n",
      "313/313 [==============================] - 70s 223ms/step - loss: 1.6277\n",
      "Epoch 40/40\n",
      "313/313 [==============================] - 69s 221ms/step - loss: 1.6252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18f0235be0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()\n",
    "        \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "steps_per_epoch = train_size // batch_size // n_steps\n",
    "model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=40,\n",
    "                   callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2,\n",
    "                     input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None, max_id]))\n",
    "stateless_model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.save('stateless.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.load_weights('stateless.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = preprocess([\"Thus the country to the str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am i have noble.\\n\\nhomes:\\ngo thee and never foundc sh'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_text(\"I am\", stateless_model, n_chars=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
