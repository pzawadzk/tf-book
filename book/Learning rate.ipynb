{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "def exponential_decay(lr0, mult):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 *  mult**epoch\n",
    "    return exponential_decay_fn\n",
    "\n",
    "mult = np.exp(np.log(1e6)/50)\n",
    "exponential_decay_fn = exponential_decay(lr0=1e-5, mult=mult)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 5s 118us/sample - loss: 14.5959 - acc: 0.5154 - val_loss: 5.6116 - val_acc: 0.7013\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 3.9902 - acc: 0.7714 - val_loss: 3.0043 - val_acc: 0.8085\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3215 - acc: 0.8426 - val_loss: 1.9832 - val_acc: 0.8550\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 1.5133 - acc: 0.8803 - val_loss: 1.4683 - val_acc: 0.8815\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 1.0215 - acc: 0.9059 - val_loss: 1.1630 - val_acc: 0.9014\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.7016 - acc: 0.9271 - val_loss: 1.0065 - val_acc: 0.9091\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.4840 - acc: 0.9408 - val_loss: 0.8380 - val_acc: 0.9195\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.3447 - acc: 0.9528 - val_loss: 0.7235 - val_acc: 0.9291\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.2760 - acc: 0.9579 - val_loss: 0.7323 - val_acc: 0.9304\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.2542 - acc: 0.9628 - val_loss: 0.6031 - val_acc: 0.9410\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.2625 - acc: 0.9621 - val_loss: 0.5836 - val_acc: 0.9402\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.2746 - acc: 0.9612 - val_loss: 0.5276 - val_acc: 0.9466\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.2898 - acc: 0.9604 - val_loss: 0.5237 - val_acc: 0.9397\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.2788 - acc: 0.9579 - val_loss: 0.4178 - val_acc: 0.9469\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.2456 - acc: 0.9576 - val_loss: 0.3195 - val_acc: 0.9505\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.2268 - acc: 0.9556 - val_loss: 0.2327 - val_acc: 0.9537\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.1927 - acc: 0.9558 - val_loss: 0.2242 - val_acc: 0.9515\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.1671 - acc: 0.9580 - val_loss: 0.1824 - val_acc: 0.9541\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 0.1612 - acc: 0.9562 - val_loss: 0.1645 - val_acc: 0.9571\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 5s 114us/sample - loss: 0.1564 - acc: 0.9587 - val_loss: 0.1564 - val_acc: 0.9615\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 5s 114us/sample - loss: 0.1813 - acc: 0.9559 - val_loss: 0.1801 - val_acc: 0.9549\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 5s 120us/sample - loss: 0.2543 - acc: 0.9413 - val_loss: 0.2340 - val_acc: 0.9471\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 5s 117us/sample - loss: 0.3908 - acc: 0.9162 - val_loss: 0.2805 - val_acc: 0.9412\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 5s 114us/sample - loss: 0.7825 - acc: 0.7964 - val_loss: 1.2766 - val_acc: 0.5635\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.2521 - acc: 0.1476 - val_loss: 2.3025 - val_acc: 0.1090\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3025 - acc: 0.1114 - val_loss: 2.3034 - val_acc: 0.1099\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 5s 114us/sample - loss: 2.3025 - acc: 0.1111 - val_loss: 2.3031 - val_acc: 0.0986\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3034 - acc: 0.1059 - val_loss: 2.3027 - val_acc: 0.1099\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 5s 114us/sample - loss: 2.3042 - acc: 0.1074 - val_loss: 2.3030 - val_acc: 0.1099\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3050 - acc: 0.1072 - val_loss: 2.3041 - val_acc: 0.1099\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 5s 114us/sample - loss: 2.3063 - acc: 0.1073 - val_loss: 2.3055 - val_acc: 0.1099\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3073 - acc: 0.1060 - val_loss: 2.3058 - val_acc: 0.1099\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3102 - acc: 0.1032 - val_loss: 2.3147 - val_acc: 0.0988\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3123 - acc: 0.1048 - val_loss: 2.3126 - val_acc: 0.1099\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 13.1528 - acc: 0.1027 - val_loss: 2.3096 - val_acc: 0.1099\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3210 - acc: 0.1033 - val_loss: 2.3156 - val_acc: 0.1012\n",
      "Epoch 37/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3289 - acc: 0.1039 - val_loss: 2.3173 - val_acc: 0.0981\n",
      "Epoch 38/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3334 - acc: 0.1037 - val_loss: 2.3177 - val_acc: 0.1012\n",
      "Epoch 39/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3412 - acc: 0.1019 - val_loss: 2.3161 - val_acc: 0.0988\n",
      "Epoch 40/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3587 - acc: 0.1018 - val_loss: 2.3487 - val_acc: 0.0986\n",
      "Epoch 41/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3705 - acc: 0.1009 - val_loss: 2.3362 - val_acc: 0.0962\n",
      "Epoch 42/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.3919 - acc: 0.1012 - val_loss: 2.3967 - val_acc: 0.0981\n",
      "Epoch 43/50\n",
      "45000/45000 [==============================] - 5s 114us/sample - loss: 2.4168 - acc: 0.1009 - val_loss: 2.5292 - val_acc: 0.0963\n",
      "Epoch 44/50\n",
      "45000/45000 [==============================] - 5s 114us/sample - loss: 2.4519 - acc: 0.0999 - val_loss: 2.4613 - val_acc: 0.1012\n",
      "Epoch 45/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.4970 - acc: 0.0997 - val_loss: 2.5846 - val_acc: 0.0986\n",
      "Epoch 46/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.5524 - acc: 0.1024 - val_loss: 2.5976 - val_acc: 0.1099\n",
      "Epoch 47/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.6099 - acc: 0.1005 - val_loss: 2.5758 - val_acc: 0.0922\n",
      "Epoch 48/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.7346 - acc: 0.0998 - val_loss: 2.6274 - val_acc: 0.0922\n",
      "Epoch 49/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 2.8473 - acc: 0.0994 - val_loss: 3.3063 - val_acc: 0.0963\n",
      "Epoch 50/50\n",
      "45000/45000 [==============================] - 5s 113us/sample - loss: 3.0491 - acc: 0.0994 - val_loss: 3.4505 - val_acc: 0.1090\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=[X_valid, y_valid], callbacks=[ lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc', 'lr'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(history.history['lr'], history.history['val_loss'])\n",
    "ax = gca()\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(history.history['lr'], history.history['val_acc'])\n",
    "ax = gca()\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/100\n",
      "45000/45000 [==============================] - 13s 286us/sample - loss: 0.3906 - acc: 0.8882 - val_loss: 0.5084 - val_acc: 0.9262\n",
      "Epoch 2/100\n",
      "45000/45000 [==============================] - 8s 168us/sample - loss: 0.2270 - acc: 0.9349 - val_loss: 0.4558 - val_acc: 0.9398\n",
      "Epoch 3/100\n",
      "45000/45000 [==============================] - 8s 168us/sample - loss: 0.1698 - acc: 0.9516 - val_loss: 0.4258 - val_acc: 0.9458\n",
      "Epoch 4/100\n",
      "45000/45000 [==============================] - 8s 167us/sample - loss: 0.1348 - acc: 0.9604 - val_loss: 0.3784 - val_acc: 0.9509\n",
      "Epoch 5/100\n",
      "45000/45000 [==============================] - 8s 168us/sample - loss: 0.1097 - acc: 0.9675 - val_loss: 0.4757 - val_acc: 0.9531\n",
      "Epoch 6/100\n",
      "45000/45000 [==============================] - 8s 168us/sample - loss: 0.0919 - acc: 0.9729 - val_loss: 0.6907 - val_acc: 0.9579\n",
      "Epoch 7/100\n",
      "45000/45000 [==============================] - 8s 168us/sample - loss: 0.0790 - acc: 0.9759 - val_loss: 0.5005 - val_acc: 0.9537\n",
      "Epoch 8/100\n",
      "45000/45000 [==============================] - 8s 168us/sample - loss: 0.0697 - acc: 0.9789 - val_loss: 0.5345 - val_acc: 0.9558\n",
      "Epoch 9/100\n",
      "45000/45000 [==============================] - 8s 167us/sample - loss: 0.0614 - acc: 0.9810 - val_loss: 0.7224 - val_acc: 0.9603\n",
      "Epoch 10/100\n",
      "45000/45000 [==============================] - 8s 167us/sample - loss: 0.0565 - acc: 0.9829 - val_loss: 0.7899 - val_acc: 0.9616\n",
      "Epoch 11/100\n",
      "45000/45000 [==============================] - 8s 168us/sample - loss: 0.0490 - acc: 0.9844 - val_loss: 0.5935 - val_acc: 0.9602\n",
      "Epoch 12/100\n",
      "45000/45000 [==============================] - 8s 167us/sample - loss: 0.0461 - acc: 0.9858 - val_loss: 0.6713 - val_acc: 0.9623\n",
      "Epoch 13/100\n",
      "45000/45000 [==============================] - 8s 168us/sample - loss: 0.0413 - acc: 0.9871 - val_loss: 0.9266 - val_acc: 0.9614\n",
      "Epoch 14/100\n",
      "45000/45000 [==============================] - 8s 170us/sample - loss: 0.0386 - acc: 0.9879 - val_loss: 0.7782 - val_acc: 0.9597\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=2e-4), metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=[X_valid, y_valid], callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9509333333333333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1*(model.predict_classes(X_valid) == y_valid).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.3398 - acc: 0.9547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3398294435672462, 0.9547]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_neurons=30, learning_rate=3e-3):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=[28, 28]))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    \n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dense(n_neurons, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "    model.add(keras.layers.Dense(n_neurons, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "              \n",
    "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_clf = keras.wrappers.scikit_learn.KerasClassifier(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 5s 215us/sample - loss: 0.3658 - acc: 0.8889 - val_loss: 0.3951 - val_acc: 0.9315\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 4s 195us/sample - loss: 0.2111 - acc: 0.9356 - val_loss: 0.4968 - val_acc: 0.9382\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 4s 195us/sample - loss: 0.1608 - acc: 0.9503 - val_loss: 0.5438 - val_acc: 0.9430\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 4s 195us/sample - loss: 0.1211 - acc: 0.9607 - val_loss: 0.5563 - val_acc: 0.9525\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 4s 195us/sample - loss: 0.1078 - acc: 0.9654 - val_loss: 0.8727 - val_acc: 0.9469\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 4s 195us/sample - loss: 0.0874 - acc: 0.9706 - val_loss: 0.8461 - val_acc: 0.9453\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 4s 195us/sample - loss: 0.0772 - acc: 0.9746 - val_loss: 0.6426 - val_acc: 0.9555\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 4s 195us/sample - loss: 0.0706 - acc: 0.9763 - val_loss: 1.2685 - val_acc: 0.9560\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 4s 195us/sample - loss: 0.0698 - acc: 0.9769 - val_loss: 0.8372 - val_acc: 0.9527\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 4s 195us/sample - loss: 0.0577 - acc: 0.9806 - val_loss: 1.1929 - val_acc: 0.9583\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 4s 195us/sample - loss: 0.0487 - acc: 0.9823 - val_loss: 1.3315 - val_acc: 0.9575\n",
      "22500/22500 [==============================] - 1s 61us/sample - loss: 0.9659 - acc: 0.9561\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 5s 222us/sample - loss: 0.3633 - acc: 0.8892 - val_loss: 0.3448 - val_acc: 0.9280\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 5s 201us/sample - loss: 0.2052 - acc: 0.9372 - val_loss: 0.4050 - val_acc: 0.9345\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 5s 201us/sample - loss: 0.1585 - acc: 0.9519 - val_loss: 0.4185 - val_acc: 0.9435\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 5s 201us/sample - loss: 0.1287 - acc: 0.9574 - val_loss: 0.3948 - val_acc: 0.9457\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 5s 201us/sample - loss: 0.1046 - acc: 0.9671 - val_loss: 0.4898 - val_acc: 0.9448\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 4s 200us/sample - loss: 0.0923 - acc: 0.9689 - val_loss: 0.5793 - val_acc: 0.9520\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 4s 199us/sample - loss: 0.0826 - acc: 0.9741 - val_loss: 0.5621 - val_acc: 0.9461\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 4s 199us/sample - loss: 0.0777 - acc: 0.9748 - val_loss: 0.6220 - val_acc: 0.9577\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 4s 198us/sample - loss: 0.0656 - acc: 0.9789 - val_loss: 0.5539 - val_acc: 0.9529\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 4s 198us/sample - loss: 0.0645 - acc: 0.9776 - val_loss: 0.6377 - val_acc: 0.9577\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 4s 199us/sample - loss: 0.0549 - acc: 0.9811 - val_loss: 0.8344 - val_acc: 0.9549\n",
      "22500/22500 [==============================] - 1s 62us/sample - loss: 0.9435 - acc: 0.9531\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 6s 253us/sample - loss: 0.4508 - acc: 0.8738 - val_loss: 0.4324 - val_acc: 0.9132\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 5s 229us/sample - loss: 0.2679 - acc: 0.9176 - val_loss: 0.6251 - val_acc: 0.9284\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 5s 232us/sample - loss: 0.1941 - acc: 0.9409 - val_loss: 0.7912 - val_acc: 0.9419\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 5s 240us/sample - loss: 0.1654 - acc: 0.9492 - val_loss: 1.1055 - val_acc: 0.9481\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 5s 232us/sample - loss: 0.1363 - acc: 0.9574 - val_loss: 0.9955 - val_acc: 0.9483\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 5s 231us/sample - loss: 0.1119 - acc: 0.9628 - val_loss: 1.3543 - val_acc: 0.9518\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 5s 231us/sample - loss: 0.1018 - acc: 0.9669 - val_loss: 0.7819 - val_acc: 0.9525\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 5s 231us/sample - loss: 0.0868 - acc: 0.9716 - val_loss: 1.3059 - val_acc: 0.9515\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 5s 231us/sample - loss: 0.0828 - acc: 0.9734 - val_loss: 1.0184 - val_acc: 0.9560\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 5s 231us/sample - loss: 0.0694 - acc: 0.9781 - val_loss: 1.1889 - val_acc: 0.9541\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 5s 231us/sample - loss: 0.0656 - acc: 0.9782 - val_loss: 1.3925 - val_acc: 0.9574\n",
      "22500/22500 [==============================] - 1s 66us/sample - loss: 0.8183 - acc: 0.9542\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 6s 254us/sample - loss: 0.4592 - acc: 0.8702 - val_loss: 0.4734 - val_acc: 0.9055\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 5s 230us/sample - loss: 0.2455 - acc: 0.9238 - val_loss: 0.5375 - val_acc: 0.9265\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 5s 229us/sample - loss: 0.2054 - acc: 0.9375 - val_loss: 0.4371 - val_acc: 0.9397\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 5s 230us/sample - loss: 0.1634 - acc: 0.9496 - val_loss: 0.4499 - val_acc: 0.9460\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 5s 228us/sample - loss: 0.1398 - acc: 0.9569 - val_loss: 0.6266 - val_acc: 0.9504\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 5s 232us/sample - loss: 0.1173 - acc: 0.9628 - val_loss: 0.5787 - val_acc: 0.9459\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 5s 231us/sample - loss: 0.1023 - acc: 0.9678 - val_loss: 0.6005 - val_acc: 0.9483\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 5s 231us/sample - loss: 0.1023 - acc: 0.9661 - val_loss: 0.6929 - val_acc: 0.9479\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 5s 231us/sample - loss: 0.0774 - acc: 0.9752 - val_loss: 0.7211 - val_acc: 0.9491\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 5s 231us/sample - loss: 0.0757 - acc: 0.9740 - val_loss: 0.4898 - val_acc: 0.9543\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 5s 232us/sample - loss: 0.0682 - acc: 0.9775 - val_loss: 0.7723 - val_acc: 0.9571\n",
      "Epoch 12/50\n",
      "22500/22500 [==============================] - 5s 231us/sample - loss: 0.0613 - acc: 0.9811 - val_loss: 0.6506 - val_acc: 0.9567\n",
      "Epoch 13/50\n",
      "22500/22500 [==============================] - 5s 232us/sample - loss: 0.0624 - acc: 0.9796 - val_loss: 0.6315 - val_acc: 0.9566\n",
      "22500/22500 [==============================] - 2s 68us/sample - loss: 1.1126 - acc: 0.9547\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 5s 232us/sample - loss: 0.3557 - acc: 0.8899 - val_loss: 0.4319 - val_acc: 0.9299\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 5s 205us/sample - loss: 0.2016 - acc: 0.9381 - val_loss: 0.4651 - val_acc: 0.9469\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 5s 204us/sample - loss: 0.1548 - acc: 0.9525 - val_loss: 0.8071 - val_acc: 0.9466\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 5s 203us/sample - loss: 0.1265 - acc: 0.9594 - val_loss: 0.6501 - val_acc: 0.9491\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 5s 203us/sample - loss: 0.1058 - acc: 0.9662 - val_loss: 0.7819 - val_acc: 0.9531\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 5s 203us/sample - loss: 0.0926 - acc: 0.9693 - val_loss: 0.8334 - val_acc: 0.9518\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 5s 203us/sample - loss: 0.0742 - acc: 0.9749 - val_loss: 1.0217 - val_acc: 0.9529\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 5s 203us/sample - loss: 0.0675 - acc: 0.9772 - val_loss: 0.7829 - val_acc: 0.9520\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 5s 203us/sample - loss: 0.0630 - acc: 0.9784 - val_loss: 1.0961 - val_acc: 0.9495\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 5s 203us/sample - loss: 0.0645 - acc: 0.9784 - val_loss: 0.8671 - val_acc: 0.9545\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 5s 204us/sample - loss: 0.0516 - acc: 0.9824 - val_loss: 1.4335 - val_acc: 0.9551\n",
      "22500/22500 [==============================] - 1s 63us/sample - loss: 1.0624 - acc: 0.9557\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 5s 234us/sample - loss: 0.3573 - acc: 0.8915 - val_loss: 0.3522 - val_acc: 0.9263\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 5s 206us/sample - loss: 0.2047 - acc: 0.9384 - val_loss: 0.3438 - val_acc: 0.9397\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 5s 220us/sample - loss: 0.1515 - acc: 0.9535 - val_loss: 0.3500 - val_acc: 0.9467\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 5s 203us/sample - loss: 0.1253 - acc: 0.9604 - val_loss: 0.3657 - val_acc: 0.9521\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 5s 204us/sample - loss: 0.1064 - acc: 0.9659 - val_loss: 0.5410 - val_acc: 0.9485\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 5s 204us/sample - loss: 0.0912 - acc: 0.9707 - val_loss: 0.4277 - val_acc: 0.9507\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 5s 203us/sample - loss: 0.0769 - acc: 0.9751 - val_loss: 0.5035 - val_acc: 0.9526\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 5s 203us/sample - loss: 0.0736 - acc: 0.9757 - val_loss: 0.6411 - val_acc: 0.9505\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 5s 204us/sample - loss: 0.0617 - acc: 0.9794 - val_loss: 0.7890 - val_acc: 0.9511\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 5s 202us/sample - loss: 0.0550 - acc: 0.9805 - val_loss: 0.6854 - val_acc: 0.9521\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 5s 201us/sample - loss: 0.0526 - acc: 0.9830 - val_loss: 0.7840 - val_acc: 0.9558\n",
      "Epoch 12/50\n",
      "22500/22500 [==============================] - 5s 201us/sample - loss: 0.0538 - acc: 0.9834 - val_loss: 0.7551 - val_acc: 0.9558\n",
      "22500/22500 [==============================] - 1s 64us/sample - loss: 0.6771 - acc: 0.9520\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 6s 252us/sample - loss: 0.4262 - acc: 0.8775 - val_loss: 0.4361 - val_acc: 0.9227\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 5s 221us/sample - loss: 0.2450 - acc: 0.9244 - val_loss: 0.5955 - val_acc: 0.9339\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 5s 221us/sample - loss: 0.1840 - acc: 0.9428 - val_loss: 1.2844 - val_acc: 0.9373\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 5s 221us/sample - loss: 0.1551 - acc: 0.9505 - val_loss: 0.5720 - val_acc: 0.9455\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 5s 224us/sample - loss: 0.1342 - acc: 0.9574 - val_loss: 0.5348 - val_acc: 0.9522\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 5s 222us/sample - loss: 0.1061 - acc: 0.9675 - val_loss: 1.1413 - val_acc: 0.9485\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 5s 221us/sample - loss: 0.0943 - acc: 0.9698 - val_loss: 0.9937 - val_acc: 0.9503\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 5s 223us/sample - loss: 0.0800 - acc: 0.9735 - val_loss: 0.9171 - val_acc: 0.9538\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 5s 224us/sample - loss: 0.0841 - acc: 0.9724 - val_loss: 0.9892 - val_acc: 0.9510\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 5s 224us/sample - loss: 0.0692 - acc: 0.9760 - val_loss: 0.9377 - val_acc: 0.9587\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 5s 224us/sample - loss: 0.0674 - acc: 0.9789 - val_loss: 1.3335 - val_acc: 0.9515\n",
      "22500/22500 [==============================] - 1s 65us/sample - loss: 0.9893 - acc: 0.9483\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 6s 253us/sample - loss: 0.4286 - acc: 0.8761 - val_loss: 0.4659 - val_acc: 0.9155\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 5s 220us/sample - loss: 0.2438 - acc: 0.9264 - val_loss: 0.5072 - val_acc: 0.9229\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 5s 220us/sample - loss: 0.1984 - acc: 0.9398 - val_loss: 0.4348 - val_acc: 0.9424\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 5s 219us/sample - loss: 0.1650 - acc: 0.9473 - val_loss: 0.4374 - val_acc: 0.9503\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 5s 220us/sample - loss: 0.1263 - acc: 0.9600 - val_loss: 0.4704 - val_acc: 0.9449\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 5s 220us/sample - loss: 0.1156 - acc: 0.9633 - val_loss: 0.4697 - val_acc: 0.9495\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 5s 220us/sample - loss: 0.0966 - acc: 0.9692 - val_loss: 0.7432 - val_acc: 0.9513\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 5s 220us/sample - loss: 0.0873 - acc: 0.9719 - val_loss: 0.7224 - val_acc: 0.9559\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 5s 220us/sample - loss: 0.0792 - acc: 0.9751 - val_loss: 0.5818 - val_acc: 0.9605\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 5s 219us/sample - loss: 0.0642 - acc: 0.9794 - val_loss: 0.7035 - val_acc: 0.9544\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 5s 220us/sample - loss: 0.0621 - acc: 0.9783 - val_loss: 1.1022 - val_acc: 0.9555\n",
      "Epoch 12/50\n",
      "22500/22500 [==============================] - 5s 220us/sample - loss: 0.0521 - acc: 0.9832 - val_loss: 0.7530 - val_acc: 0.9549\n",
      "Epoch 13/50\n",
      "22500/22500 [==============================] - 5s 220us/sample - loss: 0.0535 - acc: 0.9830 - val_loss: 0.8772 - val_acc: 0.9487\n",
      "22500/22500 [==============================] - 1s 67us/sample - loss: 0.7438 - acc: 0.9526\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 7s 293us/sample - loss: 0.4979 - acc: 0.8652 - val_loss: 0.5737 - val_acc: 0.8997\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 5s 242us/sample - loss: 0.2743 - acc: 0.9171 - val_loss: 0.7263 - val_acc: 0.9356\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 5s 242us/sample - loss: 0.2102 - acc: 0.9360 - val_loss: 0.6774 - val_acc: 0.9429\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 5s 242us/sample - loss: 0.1691 - acc: 0.9484 - val_loss: 1.0047 - val_acc: 0.9423\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 5s 242us/sample - loss: 0.1389 - acc: 0.9543 - val_loss: 1.4640 - val_acc: 0.9441\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 5s 241us/sample - loss: 0.1206 - acc: 0.9623 - val_loss: 0.6613 - val_acc: 0.9475\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 5s 242us/sample - loss: 0.0942 - acc: 0.9692 - val_loss: 1.7096 - val_acc: 0.9489\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 5s 241us/sample - loss: 0.0917 - acc: 0.9692 - val_loss: 0.9874 - val_acc: 0.9541\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 5s 241us/sample - loss: 0.0778 - acc: 0.9740 - val_loss: 0.8096 - val_acc: 0.9553\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 5s 238us/sample - loss: 0.0643 - acc: 0.9781 - val_loss: 1.7532 - val_acc: 0.9547\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 5s 238us/sample - loss: 0.0648 - acc: 0.9789 - val_loss: 1.0282 - val_acc: 0.9580\n",
      "22500/22500 [==============================] - 2s 70us/sample - loss: 0.7321 - acc: 0.9576\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 6s 277us/sample - loss: 0.4930 - acc: 0.8656 - val_loss: 0.4488 - val_acc: 0.9057\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 5s 240us/sample - loss: 0.2667 - acc: 0.9195 - val_loss: 0.4506 - val_acc: 0.9304\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 5s 239us/sample - loss: 0.2071 - acc: 0.9385 - val_loss: 0.4564 - val_acc: 0.9351\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 5s 239us/sample - loss: 0.1607 - acc: 0.9519 - val_loss: 0.5514 - val_acc: 0.9409\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 5s 239us/sample - loss: 0.1434 - acc: 0.9546 - val_loss: 0.5797 - val_acc: 0.9515\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 5s 239us/sample - loss: 0.1177 - acc: 0.9622 - val_loss: 0.5516 - val_acc: 0.9497\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 5s 239us/sample - loss: 0.0974 - acc: 0.9672 - val_loss: 0.5966 - val_acc: 0.9539\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 5s 239us/sample - loss: 0.0831 - acc: 0.9740 - val_loss: 0.6516 - val_acc: 0.9553\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 5s 238us/sample - loss: 0.0728 - acc: 0.9756 - val_loss: 0.6485 - val_acc: 0.9551\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 5s 240us/sample - loss: 0.0707 - acc: 0.9769 - val_loss: 0.5930 - val_acc: 0.9513\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 5s 239us/sample - loss: 0.0646 - acc: 0.9788 - val_loss: 0.6426 - val_acc: 0.9535\n",
      "22500/22500 [==============================] - 2s 71us/sample - loss: 0.8265 - acc: 0.9543\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 6s 278us/sample - loss: 0.4610 - acc: 0.8696 - val_loss: 0.4779 - val_acc: 0.9157\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 5s 237us/sample - loss: 0.2541 - acc: 0.9223 - val_loss: 0.5155 - val_acc: 0.9277\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 5s 236us/sample - loss: 0.1882 - acc: 0.9417 - val_loss: 0.5587 - val_acc: 0.9495\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 5s 236us/sample - loss: 0.1536 - acc: 0.9534 - val_loss: 0.9300 - val_acc: 0.9437\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 5s 235us/sample - loss: 0.1297 - acc: 0.9601 - val_loss: 0.6943 - val_acc: 0.9502\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 5s 236us/sample - loss: 0.1146 - acc: 0.9638 - val_loss: 0.7130 - val_acc: 0.9548\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 5s 235us/sample - loss: 0.1036 - acc: 0.9669 - val_loss: 1.9440 - val_acc: 0.9534\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 5s 236us/sample - loss: 0.0801 - acc: 0.9740 - val_loss: 1.4836 - val_acc: 0.9526\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 5s 237us/sample - loss: 0.0704 - acc: 0.9766 - val_loss: 0.9020 - val_acc: 0.9523\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 5s 236us/sample - loss: 0.0765 - acc: 0.9746 - val_loss: 1.2159 - val_acc: 0.9575\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 6s 252us/sample - loss: 0.0629 - acc: 0.9803 - val_loss: 1.3542 - val_acc: 0.9583\n",
      "22500/22500 [==============================] - 2s 71us/sample - loss: 0.9233 - acc: 0.9550\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 6s 275us/sample - loss: 0.4611 - acc: 0.8692 - val_loss: 0.4870 - val_acc: 0.8907\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 5s 232us/sample - loss: 0.2567 - acc: 0.9193 - val_loss: 0.4694 - val_acc: 0.9371\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 5s 230us/sample - loss: 0.1926 - acc: 0.9416 - val_loss: 0.4232 - val_acc: 0.9415\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 5s 230us/sample - loss: 0.1551 - acc: 0.9524 - val_loss: 0.4974 - val_acc: 0.9445\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 5s 231us/sample - loss: 0.1368 - acc: 0.9567 - val_loss: 0.4912 - val_acc: 0.9467\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 5s 230us/sample - loss: 0.1130 - acc: 0.9632 - val_loss: 0.5147 - val_acc: 0.9487\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 5s 230us/sample - loss: 0.1010 - acc: 0.9674 - val_loss: 0.5848 - val_acc: 0.9551\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 5s 230us/sample - loss: 0.0913 - acc: 0.9704 - val_loss: 0.7299 - val_acc: 0.9491\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 5s 233us/sample - loss: 0.0821 - acc: 0.9724 - val_loss: 1.0832 - val_acc: 0.9561\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 5s 233us/sample - loss: 0.0665 - acc: 0.9784 - val_loss: 0.8424 - val_acc: 0.9557\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 5s 234us/sample - loss: 0.0579 - acc: 0.9812 - val_loss: 0.6379 - val_acc: 0.9547\n",
      "Epoch 12/50\n",
      "22500/22500 [==============================] - 5s 233us/sample - loss: 0.0652 - acc: 0.9792 - val_loss: 0.8427 - val_acc: 0.9527\n",
      "Epoch 13/50\n",
      "22500/22500 [==============================] - 5s 234us/sample - loss: 0.0567 - acc: 0.9809 - val_loss: 1.2125 - val_acc: 0.9519\n",
      "22500/22500 [==============================] - 2s 71us/sample - loss: 1.3343 - acc: 0.9525\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 7s 305us/sample - loss: 0.3682 - acc: 0.8891 - val_loss: 0.5785 - val_acc: 0.9225\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 6s 260us/sample - loss: 0.1964 - acc: 0.9395 - val_loss: 0.5457 - val_acc: 0.9407\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 6s 259us/sample - loss: 0.1466 - acc: 0.9548 - val_loss: 0.5941 - val_acc: 0.9463\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 6s 260us/sample - loss: 0.1214 - acc: 0.9614 - val_loss: 0.7261 - val_acc: 0.9416\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 6s 259us/sample - loss: 0.1045 - acc: 0.9675 - val_loss: 0.6687 - val_acc: 0.9490\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 6s 260us/sample - loss: 0.0885 - acc: 0.9710 - val_loss: 0.9452 - val_acc: 0.9468\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 6s 259us/sample - loss: 0.0797 - acc: 0.9740 - val_loss: 0.7402 - val_acc: 0.9470\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 6s 260us/sample - loss: 0.0680 - acc: 0.9765 - val_loss: 0.7759 - val_acc: 0.9549\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 6s 259us/sample - loss: 0.0610 - acc: 0.9798 - val_loss: 0.9467 - val_acc: 0.9444\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 6s 260us/sample - loss: 0.0575 - acc: 0.9802 - val_loss: 0.6887 - val_acc: 0.9522\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 6s 259us/sample - loss: 0.0547 - acc: 0.9809 - val_loss: 0.8701 - val_acc: 0.9536\n",
      "Epoch 12/50\n",
      "22500/22500 [==============================] - 6s 259us/sample - loss: 0.0417 - acc: 0.9856 - val_loss: 0.8944 - val_acc: 0.9511\n",
      "22500/22500 [==============================] - 2s 79us/sample - loss: 0.9095 - acc: 0.9513\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 7s 309us/sample - loss: 0.3743 - acc: 0.8883 - val_loss: 0.4907 - val_acc: 0.9289\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 6s 262us/sample - loss: 0.1994 - acc: 0.9380 - val_loss: 0.4608 - val_acc: 0.9357\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 6s 262us/sample - loss: 0.1538 - acc: 0.9526 - val_loss: 0.4555 - val_acc: 0.9441\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 6s 263us/sample - loss: 0.1225 - acc: 0.9612 - val_loss: 0.4124 - val_acc: 0.9459\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 6s 276us/sample - loss: 0.1025 - acc: 0.9669 - val_loss: 0.6015 - val_acc: 0.9459\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 6s 261us/sample - loss: 0.0880 - acc: 0.9721 - val_loss: 0.5334 - val_acc: 0.9454\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 6s 259us/sample - loss: 0.0772 - acc: 0.9740 - val_loss: 0.6977 - val_acc: 0.9475\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 6s 257us/sample - loss: 0.0718 - acc: 0.9765 - val_loss: 0.5260 - val_acc: 0.9480\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 6s 257us/sample - loss: 0.0631 - acc: 0.9797 - val_loss: 0.7979 - val_acc: 0.9389\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 6s 257us/sample - loss: 0.0610 - acc: 0.9797 - val_loss: 0.6733 - val_acc: 0.9495\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 6s 258us/sample - loss: 0.0520 - acc: 0.9826 - val_loss: 0.6748 - val_acc: 0.9518\n",
      "Epoch 12/50\n",
      "22500/22500 [==============================] - 6s 258us/sample - loss: 0.0504 - acc: 0.9835 - val_loss: 0.6696 - val_acc: 0.9525\n",
      "Epoch 13/50\n",
      "22500/22500 [==============================] - 6s 258us/sample - loss: 0.0421 - acc: 0.9864 - val_loss: 0.7082 - val_acc: 0.9483\n",
      "Epoch 14/50\n",
      "22500/22500 [==============================] - 6s 257us/sample - loss: 0.0436 - acc: 0.9848 - val_loss: 0.8221 - val_acc: 0.9516\n",
      "22500/22500 [==============================] - 2s 79us/sample - loss: 0.7962 - acc: 0.9535\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 7s 300us/sample - loss: 0.5625 - acc: 0.8559 - val_loss: 0.4447 - val_acc: 0.9214\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 6s 253us/sample - loss: 0.2724 - acc: 0.9178 - val_loss: 0.5728 - val_acc: 0.9370\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 6s 253us/sample - loss: 0.1960 - acc: 0.9392 - val_loss: 0.8348 - val_acc: 0.9431\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 6s 253us/sample - loss: 0.1569 - acc: 0.9504 - val_loss: 0.6630 - val_acc: 0.9451\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 6s 253us/sample - loss: 0.1351 - acc: 0.9572 - val_loss: 0.6800 - val_acc: 0.9463\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 6s 253us/sample - loss: 0.1127 - acc: 0.9640 - val_loss: 0.7862 - val_acc: 0.9564\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 6s 254us/sample - loss: 0.0946 - acc: 0.9701 - val_loss: 0.8970 - val_acc: 0.9463\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 6s 254us/sample - loss: 0.0853 - acc: 0.9724 - val_loss: 1.8220 - val_acc: 0.9528\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 6s 254us/sample - loss: 0.0762 - acc: 0.9756 - val_loss: 1.2619 - val_acc: 0.9492\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 6s 253us/sample - loss: 0.0692 - acc: 0.9778 - val_loss: 1.2258 - val_acc: 0.9612\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 6s 254us/sample - loss: 0.0613 - acc: 0.9798 - val_loss: 1.2697 - val_acc: 0.9571\n",
      "22500/22500 [==============================] - 2s 79us/sample - loss: 1.0224 - acc: 0.9569\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 7s 307us/sample - loss: 0.5770 - acc: 0.8527 - val_loss: 0.4509 - val_acc: 0.9061\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 6s 253us/sample - loss: 0.2591 - acc: 0.9232 - val_loss: 0.7942 - val_acc: 0.9314\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 6s 254us/sample - loss: 0.1959 - acc: 0.9401 - val_loss: 0.7045 - val_acc: 0.9426\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 6s 254us/sample - loss: 0.1598 - acc: 0.9485 - val_loss: 0.4970 - val_acc: 0.9461\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 6s 254us/sample - loss: 0.1333 - acc: 0.9574 - val_loss: 0.6288 - val_acc: 0.9473\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 6s 253us/sample - loss: 0.1099 - acc: 0.9653 - val_loss: 0.6262 - val_acc: 0.9531\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 6s 254us/sample - loss: 0.0942 - acc: 0.9699 - val_loss: 0.4727 - val_acc: 0.9563\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 6s 255us/sample - loss: 0.0940 - acc: 0.9692 - val_loss: 0.6633 - val_acc: 0.9553\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 6s 270us/sample - loss: 0.0776 - acc: 0.9742 - val_loss: 0.8994 - val_acc: 0.9482\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 6s 255us/sample - loss: 0.0712 - acc: 0.9766 - val_loss: 1.1069 - val_acc: 0.9555\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 6s 253us/sample - loss: 0.0644 - acc: 0.9790 - val_loss: 1.0729 - val_acc: 0.9567\n",
      "22500/22500 [==============================] - 2s 80us/sample - loss: 0.8713 - acc: 0.9576\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 6s 265us/sample - loss: 0.3740 - acc: 0.8876 - val_loss: 0.3937 - val_acc: 0.9264\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 5s 212us/sample - loss: 0.2144 - acc: 0.9343 - val_loss: 0.4854 - val_acc: 0.9411\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 5s 211us/sample - loss: 0.1654 - acc: 0.9488 - val_loss: 0.6242 - val_acc: 0.9447\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 5s 209us/sample - loss: 0.1312 - acc: 0.9593 - val_loss: 0.5262 - val_acc: 0.9451\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.1120 - acc: 0.9656 - val_loss: 0.7057 - val_acc: 0.9495\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.0888 - acc: 0.9721 - val_loss: 0.7795 - val_acc: 0.9489\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.0791 - acc: 0.9739 - val_loss: 0.8470 - val_acc: 0.9483\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.0689 - acc: 0.9768 - val_loss: 0.8940 - val_acc: 0.9482\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.0649 - acc: 0.9777 - val_loss: 0.9050 - val_acc: 0.9573\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 5s 209us/sample - loss: 0.0560 - acc: 0.9815 - val_loss: 1.3114 - val_acc: 0.9537\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 5s 209us/sample - loss: 0.0477 - acc: 0.9841 - val_loss: 0.9651 - val_acc: 0.9558\n",
      "22500/22500 [==============================] - 2s 70us/sample - loss: 0.7827 - acc: 0.9536\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 6s 270us/sample - loss: 0.3702 - acc: 0.8880 - val_loss: 0.3526 - val_acc: 0.9241\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 5s 211us/sample - loss: 0.2160 - acc: 0.9348 - val_loss: 0.3177 - val_acc: 0.9418\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 5s 211us/sample - loss: 0.1642 - acc: 0.9510 - val_loss: 0.3458 - val_acc: 0.9459\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.1327 - acc: 0.9590 - val_loss: 0.3034 - val_acc: 0.9503\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.1072 - acc: 0.9661 - val_loss: 0.3474 - val_acc: 0.9485\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.0893 - acc: 0.9719 - val_loss: 0.4365 - val_acc: 0.9507\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.0810 - acc: 0.9743 - val_loss: 0.4838 - val_acc: 0.9505\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.0692 - acc: 0.9776 - val_loss: 0.4317 - val_acc: 0.9503\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.0595 - acc: 0.9810 - val_loss: 0.5116 - val_acc: 0.9523\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.0606 - acc: 0.9797 - val_loss: 0.4365 - val_acc: 0.9553\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 5s 209us/sample - loss: 0.0505 - acc: 0.9838 - val_loss: 0.4716 - val_acc: 0.9507\n",
      "Epoch 12/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.0442 - acc: 0.9859 - val_loss: 0.6018 - val_acc: 0.9527\n",
      "Epoch 13/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.0476 - acc: 0.9842 - val_loss: 0.5531 - val_acc: 0.9538\n",
      "Epoch 14/50\n",
      "22500/22500 [==============================] - 5s 208us/sample - loss: 0.0440 - acc: 0.9851 - val_loss: 0.4610 - val_acc: 0.9548\n",
      "22500/22500 [==============================] - 2s 70us/sample - loss: 0.6609 - acc: 0.9550\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 7s 306us/sample - loss: 0.4335 - acc: 0.8809 - val_loss: 0.4680 - val_acc: 0.9216\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 6s 246us/sample - loss: 0.2441 - acc: 0.9260 - val_loss: 0.5026 - val_acc: 0.9325\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 6s 246us/sample - loss: 0.1978 - acc: 0.9401 - val_loss: 0.7377 - val_acc: 0.9372\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 6s 246us/sample - loss: 0.1553 - acc: 0.9502 - val_loss: 0.8589 - val_acc: 0.9336\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 6s 246us/sample - loss: 0.1353 - acc: 0.9560 - val_loss: 1.2281 - val_acc: 0.9489\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 6s 263us/sample - loss: 0.1183 - acc: 0.9624 - val_loss: 0.8945 - val_acc: 0.9473\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 6s 246us/sample - loss: 0.1147 - acc: 0.9627 - val_loss: 0.8618 - val_acc: 0.9401\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 6s 248us/sample - loss: 0.0968 - acc: 0.9689 - val_loss: 0.6530 - val_acc: 0.9504\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 6s 250us/sample - loss: 0.0839 - acc: 0.9724 - val_loss: 0.6223 - val_acc: 0.9541\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 6s 250us/sample - loss: 0.0821 - acc: 0.9737 - val_loss: 0.9336 - val_acc: 0.9561\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 6s 250us/sample - loss: 0.0726 - acc: 0.9766 - val_loss: 1.0751 - val_acc: 0.9553\n",
      "22500/22500 [==============================] - 2s 81us/sample - loss: 0.8555 - acc: 0.9530\n",
      "Train on 22500 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 7s 312us/sample - loss: 0.4338 - acc: 0.8814 - val_loss: 0.3276 - val_acc: 0.9268\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 6s 250us/sample - loss: 0.2402 - acc: 0.9264 - val_loss: 0.3826 - val_acc: 0.9349\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 6s 250us/sample - loss: 0.1909 - acc: 0.9408 - val_loss: 0.5484 - val_acc: 0.9278\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 6s 250us/sample - loss: 0.1680 - acc: 0.9486 - val_loss: 0.5412 - val_acc: 0.9463\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 6s 250us/sample - loss: 0.1330 - acc: 0.9579 - val_loss: 0.5159 - val_acc: 0.9471\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 6s 251us/sample - loss: 0.1205 - acc: 0.9621 - val_loss: 0.7641 - val_acc: 0.9458\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 6s 250us/sample - loss: 0.1125 - acc: 0.9638 - val_loss: 0.7603 - val_acc: 0.9419\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 6s 250us/sample - loss: 0.1017 - acc: 0.9677 - val_loss: 0.4992 - val_acc: 0.9556\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 6s 250us/sample - loss: 0.0875 - acc: 0.9724 - val_loss: 0.7308 - val_acc: 0.9492\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 6s 250us/sample - loss: 0.0769 - acc: 0.9751 - val_loss: 0.8551 - val_acc: 0.9429\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 6s 250us/sample - loss: 0.0801 - acc: 0.9749 - val_loss: 0.5697 - val_acc: 0.9503\n",
      "22500/22500 [==============================] - 2s 82us/sample - loss: 0.8042 - acc: 0.9523\n",
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 12s 263us/sample - loss: 0.4342 - acc: 0.8848 - val_loss: 0.4895 - val_acc: 0.9329\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 10s 227us/sample - loss: 0.1890 - acc: 0.9426 - val_loss: 0.5917 - val_acc: 0.9476\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 10s 226us/sample - loss: 0.1417 - acc: 0.9563 - val_loss: 0.7764 - val_acc: 0.9562\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 10s 226us/sample - loss: 0.1160 - acc: 0.9633 - val_loss: 1.1694 - val_acc: 0.9592\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 10s 229us/sample - loss: 0.0975 - acc: 0.9699 - val_loss: 0.8706 - val_acc: 0.9631\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 10s 230us/sample - loss: 0.0841 - acc: 0.9732 - val_loss: 0.7367 - val_acc: 0.9641\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 10s 230us/sample - loss: 0.0751 - acc: 0.9764 - val_loss: 0.9574 - val_acc: 0.9647\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 11s 244us/sample - loss: 0.0650 - acc: 0.9794 - val_loss: 1.6513 - val_acc: 0.9684\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 11s 252us/sample - loss: 0.0599 - acc: 0.9814 - val_loss: 1.3683 - val_acc: 0.9680\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 11s 253us/sample - loss: 0.0627 - acc: 0.9811 - val_loss: 1.1016 - val_acc: 0.9664\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 11s 248us/sample - loss: 0.0481 - acc: 0.9845 - val_loss: 1.7963 - val_acc: 0.9689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2, error_score='raise-deprecating',\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7f5f8c0b2668>,\n",
       "                   iid='warn', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f5f9c388470>,\n",
       "                                        'n_neurons': array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 1...\n",
       "       932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944,\n",
       "       945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957,\n",
       "       958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970,\n",
       "       971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983,\n",
       "       984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996,\n",
       "       997, 998, 999])},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "\n",
    "    \"n_neurons\": np.arange(100, 1000),\n",
    "    \"learning_rate\": reciprocal(1e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_clf, param_distribs, n_iter=10, cv=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=50,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9572444558143616"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
