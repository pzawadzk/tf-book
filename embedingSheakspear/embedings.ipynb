{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@vineet.mundhra/loading-bert-with-tensorflow-hub-7f5a1c722565"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/0e/a91780d07592b1abf9c91344ce459472cc19db3b67fdf3a61dca6ebb2f5c/tensorflow_hub-0.7.0-py2.py3-none-any.whl (89kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 1.2MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_hub) (1.16.4)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_hub) (3.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_hub) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow_hub) (41.6.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.7.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b9/74c219b0310b3df0ac60c4948c4191b9377b6b746615b176819533096fb5/tensorflow_datasets-2.0.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 3.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_datasets) (2.20.0)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_datasets) (0.8.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_datasets) (1.11.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_datasets) (1.16.4)\n",
      "Requirement already satisfied: wrapt in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_datasets) (1.11.2)\n",
      "Collecting dill\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 45.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Collecting tqdm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/80/5bb262050dd2f30f8819626b7c92339708fe2ed7bd5554c8193b4487b367/tqdm-4.42.1-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 12.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_datasets) (3.8.0)\n",
      "Collecting future\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 45.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=18.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_datasets) (18.1.0)\n",
      "Collecting promise\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/9c/fb5d48abfe5d791cd496e4242ebcf87a4bb2e0c3dcd6e0ae68c11426a528/promise-2.3.tar.gz\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/aa/c4c3c9339fbe9d46edd390789d7033b4fa89e9f566d5723576dfdd3ed18e/tensorflow_metadata-0.21.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow_datasets) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.6)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow_datasets) (41.6.0)\n",
      "Collecting googleapis-common-protos\n",
      "  Downloading https://files.pythonhosted.org/packages/05/46/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf/googleapis-common-protos-1.51.0.tar.gz\n",
      "Building wheels for collected packages: dill, future, promise, googleapis-common-protos\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-cp36-none-any.whl size=77454 sha256=68895d56f71c9bcf7b6009fe8840b527dc82e61bc003367a8110bcdab38e9963\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491095 sha256=9dfaea0cb57e4eb6a118550a515dded9cfd35208158f0101cd12897107b46c2e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-cp36-none-any.whl size=20685 sha256=c8ea457149de34fd1ef76c8438fcd24961d6d0f8a084b1007031168bb640ea13\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/19/49/34/c3c1e78bcb954c49e5ec0d31784fe63d14d427f316b12fbde9\n",
      "  Building wheel for googleapis-common-protos (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for googleapis-common-protos: filename=googleapis_common_protos-1.51.0-cp36-none-any.whl size=73426 sha256=8682e966583ffa49b70841d2855e8754021356a63a7ecc0f7676d8cd53ec6ecf\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/2c/f9/7f/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\n",
      "Successfully built dill future promise googleapis-common-protos\n",
      "Installing collected packages: dill, tqdm, future, promise, googleapis-common-protos, tensorflow-metadata, tensorflow-datasets\n",
      "Successfully installed dill-0.3.1.1 future-0.18.2 googleapis-common-protos-1.51.0 promise-2.3 tensorflow-datasets-2.0.0 tensorflow-metadata-0.21.1 tqdm-4.42.1\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_hub as hub\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\" # shortcut URL\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = shakespeare_text.split()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit tokenizer needed to one-hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_vocab = list(set([bytes(v, 'utf-8') for v in text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(x):\n",
    "    return bytes_vocab.index(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.constant(np.array([b'Citizen:', b'Before', b'we', b'proceed', b'any', b'further,',\n",
    "       b'hear', b'me', b'speak.', b'All:']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {tf.constant(w):  get_index(w) for w in bytes_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lookup(x):\n",
    "    return lookup_dict[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<tf.Tensor: id=1684, shape=(), dtype=string, numpy=b'Citizen:'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-7244ae301f0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_lookup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/map_fn.py\u001b[0m in \u001b[0;36mmap_fn\u001b[0;34m(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mback_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         maximum_iterations=n)\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0mresults_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2712\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2713\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   2703\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   2704\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 2705\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2706\u001b[0m       \u001b[0mtry_to_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/map_fn.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(i, tas)\u001b[0m\n\u001b[1;32m    255\u001b[0m       \"\"\"\n\u001b[1;32m    256\u001b[0m       \u001b[0mpacked_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_ta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem_ta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melems_ta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mpacked_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m       \u001b[0mflat_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-d107397b48c9>\u001b[0m in \u001b[0;36mget_lookup\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlookup_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: <tf.Tensor: id=1684, shape=(), dtype=string, numpy=b'Citizen:'>"
     ]
    }
   ],
   "source": [
    "tf.map_fn(get_lookup, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\")\n",
    "embeddings = embed(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'First', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for d in dataset.take(1):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tf.contrib.lookup.index_table_from_tensor(\n",
    "    mapping=bytes_vocab, default_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda windows: (windows[:-1], windows[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=278803, shape=(10,), dtype=string, numpy=\n",
      "array([b'First', b'Citizen:', b'Before', b'we', b'proceed', b'any',\n",
      "       b'further,', b'hear', b'me', b'speak.'], dtype=object)>, <tf.Tensor: id=278804, shape=(10,), dtype=string, numpy=\n",
      "array([b'Citizen:', b'Before', b'we', b'proceed', b'any', b'further,',\n",
      "       b'hear', b'me', b'speak.', b'All:'], dtype=object)>)\n"
     ]
    }
   ],
   "source": [
    "for d in dataset.take(1):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (embed(X_batch), tf.one_hot(table.lookup(Y_batch), \n",
    "                     len(bytes_vocab), dtype=tf.int8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset.take(1):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. One hot encode Y\n",
    "# 2. Encode stop words, Comas, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=250, shape=(32, 10, 50), dtype=float32, numpy=\n",
      "array([[[ 1.58649027e-01,  1.28589392e-01, -1.29613027e-01, ...,\n",
      "         -1.01859465e-01, -3.05732191e-01, -1.98210493e-01],\n",
      "        [-2.56005861e-02, -5.26675545e-02,  3.39039713e-02, ...,\n",
      "         -5.28172441e-02, -1.56363156e-02, -4.26248722e-02],\n",
      "        [ 8.79037604e-02,  1.23868331e-01,  1.46265309e-02, ...,\n",
      "          5.23213763e-03,  2.99760997e-02,  4.18323092e-02],\n",
      "        ...,\n",
      "        [-1.87684625e-01,  1.11483440e-01, -6.98339492e-02, ...,\n",
      "         -8.72121304e-02, -8.81422907e-02, -1.21769913e-01],\n",
      "        [-2.02914149e-01, -1.01657093e-01,  1.79136079e-02, ...,\n",
      "         -3.63735557e-02, -5.38155087e-04,  9.52688009e-02],\n",
      "        [-1.33177489e-01,  9.86592174e-02, -4.51362878e-01, ...,\n",
      "         -1.23879559e-01, -5.18544391e-02,  6.37197196e-02]],\n",
      "\n",
      "       [[-2.51807515e-02,  3.18714857e-01,  6.77748024e-03, ...,\n",
      "          1.11583963e-01, -3.34233195e-01, -4.43047024e-02],\n",
      "        [ 3.23567688e-01, -1.01629056e-01, -1.42747480e-02, ...,\n",
      "         -1.64461490e-02, -4.04298335e-01,  6.44795522e-02],\n",
      "        [-9.81969908e-02,  2.05971986e-01,  6.23969957e-02, ...,\n",
      "         -3.64919975e-02,  4.86329980e-02, -9.90669951e-02],\n",
      "        ...,\n",
      "        [ 2.05430686e-01,  4.76998866e-01, -1.20247409e-01, ...,\n",
      "         -1.92275807e-01, -1.26102537e-01, -1.28029779e-01],\n",
      "        [ 6.28763717e-03,  1.30490020e-01,  3.66168410e-01, ...,\n",
      "          9.65789184e-02, -2.32878789e-01, -1.74160838e-01],\n",
      "        [ 1.70245782e-01, -1.13577634e-01,  2.56983731e-02, ...,\n",
      "          1.56414941e-01, -4.28789854e-03,  1.24866404e-01]],\n",
      "\n",
      "       [[-8.69620517e-02,  2.09187582e-01,  5.58831245e-02, ...,\n",
      "         -1.60659567e-01,  4.80921194e-02,  1.39107943e-01],\n",
      "        [ 1.53995574e-01,  2.94703156e-01,  3.24788056e-02, ...,\n",
      "         -5.98829277e-02,  7.22644292e-03, -7.45810047e-02],\n",
      "        [ 1.36133596e-01,  4.37508412e-02,  2.10143387e-01, ...,\n",
      "         -5.29416949e-02, -2.45320469e-01, -8.00301656e-02],\n",
      "        ...,\n",
      "        [ 5.85223079e-01, -8.58199149e-02,  5.76301441e-02, ...,\n",
      "          1.98292024e-02,  3.88167575e-02, -3.16394746e-01],\n",
      "        [-5.56080826e-02,  7.53754973e-02,  5.69627173e-02, ...,\n",
      "         -1.43939769e-02,  7.80446902e-02,  1.06227472e-01],\n",
      "        [ 5.49398482e-01,  4.01323475e-02, -1.37453347e-01, ...,\n",
      "          9.65237245e-02,  2.60144472e-02,  2.37557933e-01]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 2.18083963e-01, -2.87737221e-01, -1.91302389e-01, ...,\n",
      "          5.18284626e-02,  1.67036101e-01, -2.79471278e-01],\n",
      "        [ 2.78086722e-01, -2.59782188e-03, -1.13719925e-01, ...,\n",
      "          1.49915963e-01,  1.45127103e-01,  6.22148961e-02],\n",
      "        [ 1.45145550e-01,  7.97550008e-02,  2.60698348e-01, ...,\n",
      "          1.66369349e-01, -1.66745663e-01, -1.69000417e-01],\n",
      "        ...,\n",
      "        [ 3.57788920e-01,  1.73769116e-01, -1.76979434e-02, ...,\n",
      "          2.97452301e-01, -1.57745019e-01,  2.63086464e-02],\n",
      "        [-3.70081030e-02, -1.07808530e-01, -9.47812274e-02, ...,\n",
      "          1.25979288e-02,  2.59425063e-02,  1.73029095e-01],\n",
      "        [-4.56173904e-02, -1.38334900e-01, -3.91151682e-02, ...,\n",
      "         -6.27624840e-02,  1.37253404e-01, -8.72092098e-02]],\n",
      "\n",
      "       [[ 1.27946287e-01,  1.70224786e-01, -1.02870226e-01, ...,\n",
      "         -1.69669896e-01, -1.53759137e-01,  1.24078453e-01],\n",
      "        [-1.87684625e-01,  1.11483440e-01, -6.98339492e-02, ...,\n",
      "         -8.72121304e-02, -8.81422907e-02, -1.21769913e-01],\n",
      "        [ 1.08342975e-01, -3.28237377e-02, -5.80524877e-02, ...,\n",
      "          8.61918647e-03,  1.86492145e-01, -7.46519945e-04],\n",
      "        ...,\n",
      "        [ 5.19539535e-01, -1.06691569e-01, -1.60531312e-01, ...,\n",
      "          1.83669839e-03, -1.32945299e-01, -3.93212400e-02],\n",
      "        [-5.56080826e-02,  7.53754973e-02,  5.69627173e-02, ...,\n",
      "         -1.43939769e-02,  7.80446902e-02,  1.06227472e-01],\n",
      "        [ 1.88241750e-01,  2.39480902e-02, -5.42743206e-02, ...,\n",
      "         -5.37840836e-02, -8.22400898e-02, -4.84659113e-02]],\n",
      "\n",
      "       [[ 1.47286594e-01,  1.07417054e-01,  1.17765814e-01, ...,\n",
      "          1.56394318e-02, -1.48981169e-01, -5.42020686e-02],\n",
      "        [ 6.28763717e-03,  1.30490020e-01,  3.66168410e-01, ...,\n",
      "          9.65789184e-02, -2.32878789e-01, -1.74160838e-01],\n",
      "        [ 1.79814726e-01, -1.11502685e-01,  1.17016897e-01, ...,\n",
      "          5.22433408e-03,  2.06607115e-02, -8.71925801e-02],\n",
      "        ...,\n",
      "        [-9.81969908e-02,  2.05971986e-01,  6.23969957e-02, ...,\n",
      "         -3.64919975e-02,  4.86329980e-02, -9.90669951e-02],\n",
      "        [ 2.98986197e-01,  5.54242358e-02, -1.30247921e-01, ...,\n",
      "          1.23326950e-01,  4.30803251e-04, -8.24358016e-02],\n",
      "        [ 1.04817063e-01,  1.30728081e-01,  3.67961198e-01, ...,\n",
      "         -1.14513062e-01, -2.63227135e-01, -6.31870404e-02]]],\n",
      "      dtype=float32)>, <tf.Tensor: id=251, shape=(32, 10), dtype=string, numpy=\n",
      "array([[b'not', b'arms,', b'must', b'help.', b'Alack,', b'You', b'are',\n",
      "        b'transported', b'by', b'calamity'],\n",
      "       [b'you.', b'First', b'Citizen:', b'Your', b\"belly's\", b'answer?',\n",
      "        b'What!', b'The', b'kingly-crowned', b'head,'],\n",
      "       [b'this', b'our', b'fabric,', b'if', b'that', b'they--',\n",
      "        b'MENENIUS:', b'What', b'then?', b\"'Fore\"],\n",
      "       [b'What', b'then?', b\"'Fore\", b'me,', b'this', b'fellow',\n",
      "        b'speaks!', b'What', b'then?', b'what'],\n",
      "       [b'state,', b'who', b'care', b'for', b'you', b'like', b'fathers,',\n",
      "        b'When', b'you', b'curse'],\n",
      "       [b'our', b'sufferance', b'is', b'a', b'gain', b'to', b'them',\n",
      "        b'Let', b'us', b'revenge'],\n",
      "       [b'this', b'fellow', b'speaks!', b'What', b'then?', b'what',\n",
      "        b'then?', b'First', b'Citizen:', b'Should'],\n",
      "       [b'Menenius', b'Agrippa;', b'one', b'that', b'hath', b'always',\n",
      "        b'loved', b'the', b'people.', b'First'],\n",
      "       [b'mother', b'and', b'to', b'be', b'partly', b'proud;', b'which',\n",
      "        b'he', b'is,', b'even'],\n",
      "       [b'Sir,', b'I', b'shall', b'tell', b'you.', b'With', b'a',\n",
      "        b'kind', b'of', b'smile,'],\n",
      "       [b'did', b'it', b'to', b'please', b'his', b'mother', b'and',\n",
      "        b'to', b'be', b'partly'],\n",
      "       [b'body.', b'The', b'belly', b\"answer'd--\", b'First', b'Citizen:',\n",
      "        b'Well,', b'sir,', b'what', b'answer'],\n",
      "       [b'is', b'as', b'an', b'inventory', b'to', b'particularise',\n",
      "        b'their', b'abundance;', b'our', b'sufferance'],\n",
      "       [b'Of', b'the', b'whole', b'body.', b'The', b'belly',\n",
      "        b\"answer'd--\", b'First', b'Citizen:', b'Well,'],\n",
      "       [b'the', b'mutinous', b'parts', b'That', b'envied', b'his',\n",
      "        b'receipt;', b'even', b'so', b'most'],\n",
      "       [b'counsellor', b'heart,', b'the', b'arm', b'our', b'soldier,',\n",
      "        b'Our', b'steed', b'the', b'leg,'],\n",
      "       [b'us', b'humanely;', b'but', b'they', b'think', b'we', b'are',\n",
      "        b'too', b'dear:', b'the'],\n",
      "       [b'Of', b'the', b'whole', b'body.', b'The', b'belly',\n",
      "        b\"answer'd--\", b'First', b'Citizen:', b'Well,'],\n",
      "       [b'with', b'the', b'rest,', b'where', b'the', b'other',\n",
      "        b'instruments', b'Did', b'see', b'and'],\n",
      "       [b'All:', b'We', b\"know't,\", b'we', b\"know't.\", b'First',\n",
      "        b'Citizen:', b'Let', b'us', b'kill'],\n",
      "       [b'surplus,', b'to', b'tire', b'in', b'repetition.', b'What',\n",
      "        b'shouts', b'are', b'these?', b'The'],\n",
      "       [b'especially', b'against', b'Caius', b'Marcius?', b'All:',\n",
      "        b'Against', b'him', b'first:', b\"he's\", b'a'],\n",
      "       [b'What', b'could', b'the', b'belly', b'answer?', b'MENENIUS:',\n",
      "        b'I', b'will', b'tell', b'you'],\n",
      "       [b'good', b'report', b'fort,', b'but', b'that', b'he', b'pays',\n",
      "        b'himself', b'with', b'being'],\n",
      "       [b'to', b'support', b'usurers;', b'repeal', b'daily', b'any',\n",
      "        b'wholesome', b'act', b'established', b'against'],\n",
      "       [b'Patience', b'awhile,', b\"you'll\", b'hear', b'the', b\"belly's\",\n",
      "        b'answer.', b'First', b'Citizen:', b\"Ye're\"],\n",
      "       [b'suitors', b'have', b'strong', b'breaths:', b'they', b'shall',\n",
      "        b'know', b'we', b'have', b'strong'],\n",
      "       [b'as', b'well', b'Strike', b'at', b'the', b'heaven', b'with',\n",
      "        b'your', b'staves', b'as'],\n",
      "       [b'are', b'not', b'such', b'as', b'you.', b'First', b'Citizen:',\n",
      "        b'Your', b\"belly's\", b'answer?'],\n",
      "       [b\"'t\", b'a', b'little', b'more.', b'First', b'Citizen:',\n",
      "        b'Well,', b\"I'll\", b'hear', b'it,'],\n",
      "       [b'are', b'accounted', b'poor', b'citizens,', b'the',\n",
      "        b'patricians', b'good.', b'What', b'authority', b'surfeits'],\n",
      "       [b'The', b'matter?', b'speak,', b'I', b'pray', b'you.', b'First',\n",
      "        b'Citizen:', b'Our', b'business']], dtype=object)>)\n"
     ]
    }
   ],
   "source": [
    "for d in dataset.take(1):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-38-62e6d67f2ffd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-62e6d67f2ffd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    dataset = dataset.map(lambda (X, y): (embed(X), embed(y)))\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda (X, y): (embed(X), embed(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "#model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "#history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=4)\n",
    "#model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
