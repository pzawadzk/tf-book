{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-47-b3d768f13ecc>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-b3d768f13ecc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Failure reason\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Failure reason\n",
    "No objective metrics found after running 5 training jobs. Please ensure that the custom algorithm is emitting the objective metric as defined by the regular expression provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New API:\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html\n",
    "\n",
    "\n",
    "https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/tf-distribution-options/tf-distributed-training.ipynb\n",
    "https://medium.com/radix-ai-blog/tensorflow-sagemaker-d17774417f08\n",
    "\n",
    "https://blog.betomorrow.com/keras-in-the-cloud-with-amazon-sagemaker-67cf11fb536\n",
    "\n",
    "https://github.com/aws/sagemaker-python-sdk/tree/v1.12.0/src/sagemaker/tensorflow#tensorflow-sagemaker-estimators-and-models\n",
    "\n",
    "https://aws.amazon.com/blogs/machine-learning/train-and-deploy-keras-models-with-tensorflow-and-apache-mxnet-on-amazon-sagemaker/\n",
    "\n",
    "\n",
    "Convert keras estimator to https://stackoverflow.com/questions/51455863/whats-the-difference-between-a-tensorflow-keras-model-and-estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw npz data and write shuffled data to TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = [], [], []\n",
    "classes = []\n",
    "count = 0\n",
    "for file in glob.glob('sketchrnn_data/*npz'):\n",
    "    if \"full\" not in file:\n",
    "        data = np.load(file, encoding='bytes', allow_pickle=True)\n",
    "        class_name = re.findall(r'(\\w+).npz', file)[0]\n",
    "        train.append(data['train'])\n",
    "        valid.append(data['valid'])\n",
    "        test.append(data['test'])\n",
    "        classes.append(class_name)\n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            break\n",
    "X_train_raw = np.concatenate(train)\n",
    "X_valid_raw = np.concatenate(valid)\n",
    "X_test_raw = np.concatenate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(classes)\n",
    "\n",
    "train_mask = np.arange(700000)\n",
    "np.random.shuffle(train_mask)\n",
    "\n",
    "valid_mask = np.arange(25000)\n",
    "np.random.shuffle(valid_mask)\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train_raw, padding='post', value=99, maxlen=160)[train_mask]\n",
    "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid_raw, padding='post', value=99, maxlen=160)[valid_mask]\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test_raw, padding='post', value=99, maxlen=160)\n",
    "\n",
    "y_train = le.transform(np.repeat(classes, 70000))[train_mask]\n",
    "y_valid = le.transform(np.repeat(classes, 2500))[valid_mask]\n",
    "y_test = le.transform(np.repeat(classes, 2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _int64_feature_list(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def convert_dataset(X, y, name, directory):\n",
    "\n",
    "    filename = os.path.join(directory, name + '.tfrecords')\n",
    "    print(f'Writing {filename}')\n",
    "    \n",
    "    with tf.python_io.TFRecordWriter(filename) as writer: \n",
    "        for i in range(len(y)):\n",
    "                example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'feature': _int64_feature_list(X[i].reshape(-1)),\n",
    "                'label': _int64_feature(y[i])\n",
    "                }))\n",
    "                writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./sketchTF/train.tfrecords\n",
      "Writing ./sketchTF/valid.tfrecords\n"
     ]
    }
   ],
   "source": [
    "convert_dataset(X_train, y_train, \"train\", \"./sketchTF\")\n",
    "convert_dataset(X_valid, y_valid, \"valid\", \"./sketchTF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safe data to TF record from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names\n",
    "classes = []\n",
    "files = []\n",
    "count = 0\n",
    "for file in glob.glob('sketchrnn_data/*npz'):\n",
    "    if \"full\" not in file:\n",
    "        class_name = re.findall(r'(\\w+).npz', file)[0]\n",
    "        classes.append(class_name)\n",
    "        files.append(file)\n",
    "        count += 1\n",
    "    if count == 10:\n",
    "        break\n",
    "files_to_int = {c: i for i, c in enumerate(files)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_dataset_from_files(files, name, directory):\n",
    "\n",
    "    filename = os.path.join(directory, name + '.tfrecords')\n",
    "    print(f'Writing {filename}')\n",
    "    \n",
    "    with tf.python_io.TFRecordWriter(filename) as writer: \n",
    "        for file in files:\n",
    "            data = np.load(file, encoding='bytes', allow_pickle=True)\n",
    "            extracted_data = tf.keras.preprocessing.sequence.pad_sequences(data[name], padding='post', value=99, maxlen=160) \n",
    "            for data_point in extracted_data:\n",
    "                example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'feature': _int64_feature_list(data_point.reshape(-1)),\n",
    "                'label': _int64_feature(files_to_int[file])\n",
    "                }))\n",
    "                writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./sketchTF/train.tfrecords\n",
      "Writing ./sketchTF/test.tfrecords\n",
      "Writing ./sketchTF/valid.tfrecords\n"
     ]
    }
   ],
   "source": [
    "for mode in ['train', 'test', 'valid']:\n",
    "    convert_dataset_from_files(files, mode, \"./sketchTF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "features={\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'feature': tf.FixedLenFeature([480], tf.int64)\n",
    "        }\n",
    "\n",
    "for serialized_example in tf.data.TFRecordDataset([\"sketchTF/train.tfrecords\"]):\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example, features)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=76, shape=(160, 3), dtype=int64, numpy=\n",
       "array([[  10,   -5,    0],\n",
       "       [  82,   -9,    0],\n",
       "       [ 195,  -16,    0],\n",
       "       [ 117,    4,    0],\n",
       "       [  59,   -2,    0],\n",
       "       [  42,    7,    0],\n",
       "       [  27,   10,    0],\n",
       "       [   8,   10,    0],\n",
       "       [   4,   19,    0],\n",
       "       [   1,  129,    0],\n",
       "       [  -8,   17,    0],\n",
       "       [  -9,    6,    0],\n",
       "       [ -46,   10,    0],\n",
       "       [-264,    1,    0],\n",
       "       [ -85,  -12,    0],\n",
       "       [ -60,  -21,    0],\n",
       "       [ -19,  -10,    0],\n",
       "       [ -17,  -14,    0],\n",
       "       [ -14,  -16,    0],\n",
       "       [  -9,  -16,    0],\n",
       "       [ -16,  -73,    1],\n",
       "       [ 101,   17,    0],\n",
       "       [ -19,    4,    0],\n",
       "       [  -4,   10,    0],\n",
       "       [   3,   16,    0],\n",
       "       [  11,   18,    0],\n",
       "       [  16,   12,    0],\n",
       "       [  13,    1,    0],\n",
       "       [  17,   -9,    0],\n",
       "       [  16,  -18,    0],\n",
       "       [   3,  -19,    0],\n",
       "       [  -6,  -20,    0],\n",
       "       [ -16,  -14,    0],\n",
       "       [ -18,   -4,    0],\n",
       "       [ -35,   11,    0],\n",
       "       [  -8,    8,    0],\n",
       "       [  -1,    6,    0],\n",
       "       [  17,   11,    1],\n",
       "       [  87,   -1,    0],\n",
       "       [   0,   16,    0],\n",
       "       [  12,   12,    0],\n",
       "       [  40,    4,    0],\n",
       "       [  16,   -7,    0],\n",
       "       [  13,  -13,    0],\n",
       "       [   0,  -15,    0],\n",
       "       [ -11,  -19,    0],\n",
       "       [ -18,  -17,    0],\n",
       "       [ -20,   -6,    0],\n",
       "       [ -20,    5,    0],\n",
       "       [ -14,    9,    0],\n",
       "       [  -5,    9,    0],\n",
       "       [   4,    5,    0],\n",
       "       [  27,    9,    1],\n",
       "       [  94,  -13,    0],\n",
       "       [ -12,   -3,    0],\n",
       "       [ -13,    3,    0],\n",
       "       [  -8,    7,    0],\n",
       "       [   1,   11,    0],\n",
       "       [  13,   15,    0],\n",
       "       [  18,   11,    0],\n",
       "       [  20,    3,    0],\n",
       "       [  18,   -2,    0],\n",
       "       [  11,   -6,    0],\n",
       "       [   4,   -8,    0],\n",
       "       [  -4,  -16,    0],\n",
       "       [ -13,  -13,    0],\n",
       "       [ -17,  -11,    0],\n",
       "       [ -15,   -2,    0],\n",
       "       [ -11,    4,    0],\n",
       "       [  -5,    6,    0],\n",
       "       [   4,    7,    0],\n",
       "       [  17,    9,    1],\n",
       "       [ 107,   -2,    0],\n",
       "       [ -24,    6,    0],\n",
       "       [   0,   10,    0],\n",
       "       [   7,   14,    0],\n",
       "       [  12,    8,    0],\n",
       "       [  15,    2,    0],\n",
       "       [  17,   -3,    0],\n",
       "       [  12,   -9,    0],\n",
       "       [   6,  -10,    0],\n",
       "       [   3,  -23,    0],\n",
       "       [  -7,  -19,    0],\n",
       "       [ -14,  -16,    0],\n",
       "       [ -17,  -10,    0],\n",
       "       [ -34,   -2,    1],\n",
       "       [ -64,  -13,    0],\n",
       "       [   0,  -31,    0],\n",
       "       [  -9,  -78,    0],\n",
       "       [ -11,  -46,    0],\n",
       "       [  -2,  -26,    0],\n",
       "       [   3,  -25,    0],\n",
       "       [   6,   -3,    0],\n",
       "       [  11,    1,    0],\n",
       "       [  32,   11,    0],\n",
       "       [  37,    7,    0],\n",
       "       [  12,    8,    0],\n",
       "       [   6,  116,    0],\n",
       "       [  12,   52,    0],\n",
       "       [  17,   36,    1],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99],\n",
       "       [  99,   99,   99]])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(parsed_example['feature'], [160, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_example['feature'].reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-549525720196/sketchTF/data/10classShuffle\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "bucket = sagemaker.Session().default_bucket() # Automatically create a bucket\n",
    "prefix = 'sketchTF' # Subfolder prefix\n",
    "s3_url = sagemaker.Session().upload_data(path='sketchTF', \n",
    "                                         bucket=bucket, \n",
    "                                         key_prefix=prefix+'/data/10classShuffle')\n",
    "print(s3_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-17 17:33:57   23664843 test.tfrecords\n",
      "2020-01-17 17:33:59  663005344 train.tfrecords\n",
      "2020-01-17 17:34:06   23653600 valid.tfrecords\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://sagemaker-us-west-2-549525720196/sketchTF/data/10classShuffle/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sagmaker Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:tensorflow py2 container will be deprecated soon.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "role = sagemaker.get_execution_role() \n",
    "#\"ml.p3.2xlarge\",\n",
    "train_instance_type='ml.p3.2xlarge'      # The type of EC2 instance which will be used for training\n",
    "deploy_instance_type='ml.p2.xlarge'     # The type of EC2 instance which will be used for deployment\n",
    "hyperparameters={\n",
    "#    \"learning_rate\": 1e-4,\n",
    "    \"n_hidden\": 1,\n",
    "    \"n_neurons\": 50\n",
    "}\n",
    "\n",
    "estimator = TensorFlow(entry_point='sketchRNN_keras.py',\n",
    "                          role=role,\n",
    "                          framework_version=\"1.12.0\",               # TensorFlow's version\n",
    "                          input_mode='Pipe',\n",
    "                          hyperparameters=hyperparameters,\n",
    "                          training_steps=1000,\n",
    "                          evaluation_steps=100,\n",
    "                          train_instance_count=2,                   # \"The number of GPUs instances to use\"\n",
    "                          train_instance_type=train_instance_type\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-01-17 17:55:39 Starting - Starting the training job.....\n",
      "2020-01-17 17:56:09 Starting - Launching requested ML instances..................\n",
      "2020-01-17 17:57:41 Starting - Preparing the instances for training...........\n",
      "2020-01-17 17:58:45 Downloading - Downloading input data....\n",
      "2020-01-17 17:59:08 Training - Downloading the training image...\n",
      "2020-01-17 17:59:28 Training - Training image download completed. Training in progress........................................................................................................................................................................................................\n",
      "2020-01-17 18:16:13 Uploading - Uploading generated training model\n",
      "2020-01-17 18:16:20 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "train_data = 's3://sagemaker-us-west-2-549525720196/sketchTF/data/10classShuffle/train.tfrecords'\n",
    "eval_data = 's3://sagemaker-us-west-2-549525720196/sketchTF/data/10classShuffle/valid.tfrecords'\n",
    "\n",
    "estimator.fit({'train': train_data, 'eval': eval_data}, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# git_config = {'repo': 'https://github.com/aws-samples/amazon-sagemaker-script-mode', 'branch': 'master'}\n",
    "\n",
    "ps_instance_type = 'ml.p3.2xlarge'\n",
    "ps_instance_count = 1\n",
    "\n",
    "model_dir = \"/opt/ml/model\"\n",
    "\n",
    "distributions = {'parameter_server': {\n",
    "                    'enabled': True}\n",
    "                }\n",
    "hyperparameters = {'epochs': 10, 'batch-size' : 128}\n",
    "\n",
    "role = sagemaker.get_execution_role() \n",
    "\n",
    "estimator_ps = TensorFlow(\n",
    "                       #git_config=git_config,\n",
    "                       source_dir='code',\n",
    "                       entry_point='main.py', \n",
    "                       base_job_name='sketchRNN',\n",
    "                       role=role,\n",
    "                       input_mode='Pipe',\n",
    "                       framework_version='1.13',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       train_instance_count=ps_instance_count, \n",
    "                       train_instance_type=ps_instance_type,\n",
    "                       model_dir=model_dir,\n",
    "                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'dist'}],\n",
    "                       distributions=distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-01-22 16:59:32 Starting - Starting the training job\n",
      "2020-01-22 16:59:33 Starting - Launching requested ML instances.................\n",
      "2020-01-22 17:01:05 Starting - Preparing the instances for training.............\n",
      "2020-01-22 17:02:14 Downloading - Downloading input data\n",
      "2020-01-22 17:02:21 Training - Downloading the training image...\n",
      "2020-01-22 17:02:41 Training - Training image download completed. Training in progress................................................................................\n",
      "2020-01-22 17:09:22 Uploading - Uploading generated training model\n",
      "2020-01-22 17:09:28 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "train_data = 's3://sagemaker-us-west-2-549525720196/sketchTF/data/10classShuffle/train.tfrecords'\n",
    "eval_data = 's3://sagemaker-us-west-2-549525720196/sketchTF/data/10classShuffle/valid.tfrecords'\n",
    "\n",
    "estimator_ps.fit({'train': train_data, 'eval': eval_data}, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-549525720196/sketchRNN-2020-01-22-16-59-31-334/output/model.tar.gz to ps_model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "! aws s3 cp {estimator_ps.model_data} ./ps_model/model.tar.gz\n",
    "! tar -xzf ./ps_model/model.tar.gz -C ./ps_model\n",
    "\n",
    "with open('./ps_model/ps_history.p', \"r\") as f:\n",
    "    ps_history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.27544069290161133,\n",
       "  0.5592948794364929,\n",
       "  0.6362179517745972,\n",
       "  0.6660656929016113,\n",
       "  0.6666666865348816,\n",
       "  0.6891025900840759,\n",
       "  0.7059294581413269,\n",
       "  0.7049278616905212,\n",
       "  0.7279647588729858,\n",
       "  0.7261618375778198],\n",
       " 'loss': [2.0889766185711593,\n",
       "  1.3086259487347724,\n",
       "  1.0669012115551875,\n",
       "  0.988345803358616,\n",
       "  0.9500429248198484,\n",
       "  0.9061867136221665,\n",
       "  0.8732469448676476,\n",
       "  0.8524767221548618,\n",
       "  0.7947041621574988,\n",
       "  0.7921125720708798],\n",
       " 'val_acc': [0.4314236044883728,\n",
       "  0.6050347089767456,\n",
       "  0.6692708134651184,\n",
       "  0.65625,\n",
       "  0.6944444179534912,\n",
       "  0.7465277910232544,\n",
       "  0.6770833134651184,\n",
       "  0.6883680820465088,\n",
       "  0.7265625,\n",
       "  0.7491319179534912],\n",
       " 'val_loss': [1.6530858278274536,\n",
       "  1.1719423797395494,\n",
       "  1.0111279222700331,\n",
       "  0.9839720129966736,\n",
       "  0.9122723076078627,\n",
       "  0.7926922308074104,\n",
       "  0.9757408764627244,\n",
       "  0.8585047324498495,\n",
       "  0.7984801663292779,\n",
       "  0.7620407342910767]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"TA: 1:40 - loss: 0.8548 - acc: 0.7031#010#010#010#010#010#010#010#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.findall(, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.8548']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.parameter import (\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    IntegerParameter,\n",
    "    ParameterRange,\n",
    ")\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "# Define objective\n",
    "objective_metric_name = 'acc'\n",
    "objective_type = 'Minimize'\n",
    "metric_definitions = [{'Name': 'acc',\n",
    "                       'Regex': 'val_acc: ([0-9\\\\.]+)'}]\n",
    "# Define hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "                        #'learning_rate': ContinuousParameter(0.0001, 0.01, scaling_type='Logarithmic'), \n",
    "                            #'dropout_rate': ContinuousParameter(0.3, 1.0),\n",
    "                            'n_hidden': IntegerParameter(1, 4),\n",
    "                            'n_neurons': IntegerParameter(10, 100),\n",
    "                            #'optimizer_type': CategoricalParameter(['sgd', 'adam']),\n",
    "                        }  \n",
    "# Initialise Sagemaker's hyperparametertuner\n",
    "tuner = HyperparameterTuner(estimator_ps,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=20,\n",
    "                            max_parallel_jobs=2,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': train_data, 'eval': eval_data}, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a classfication model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "tf.keras.layers.Masking(mask_value=99, input_shape=(None, 3)),\n",
    "keras.layers.LSTM(50, return_sequences=True),\n",
    "keras.layers.LSTM(50, return_sequences=True),\n",
    "keras.layers.LSTM(50),\n",
    "keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, None, 3)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 50)          10800     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 50)          20200     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 51,710\n",
      "Trainable params: 51,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 576s 58ms/sample - loss: 1.4292 - acc: 0.5032 - val_loss: 1.0432 - val_acc: 0.6525\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 571s 57ms/sample - loss: 0.9262 - acc: 0.6809 - val_loss: 0.8853 - val_acc: 0.7025\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 566s 57ms/sample - loss: 0.7979 - acc: 0.7319 - val_loss: 0.8226 - val_acc: 0.7157\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 568s 57ms/sample - loss: 0.7251 - acc: 0.7525 - val_loss: 0.7786 - val_acc: 0.7371\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 569s 57ms/sample - loss: 0.6519 - acc: 0.7829 - val_loss: 0.7146 - val_acc: 0.7605\n",
      "Epoch 6/10\n",
      " 9984/10000 [============================>.] - ETA: 0s - loss: 0.6156 - acc: 0.7920"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-4f630f504829>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m           \u001b[0mvalidation_in_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m           \u001b[0mprepared_feed_values_from_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    441\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N=10000\n",
    "model.fit(X_train[:N], y_train[:N], epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
